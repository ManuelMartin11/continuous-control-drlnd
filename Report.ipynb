{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import copy\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726671e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.9399999789893627\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use DDPG Actor-Critic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    in_fan = layer.weight.data.size()[0]\n",
    "    return (-1./np.sqrt(in_fan), 1./np.sqrt(in_fan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Build Actor Critic Neural Networks Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD FIRST THE ACTOR CRITIC MODEL\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=256):\n",
    "        \"\"\"\n",
    "        :param: state_size\n",
    "        :param: action_size\n",
    "        :param: seed\n",
    "        :param: fc1_units\n",
    "        :param: fc2_units\n",
    "        \"\"\"\n",
    "        \n",
    "#Actor and Critic Networks\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        :param: state_size: Dimension of each state\n",
    "        :param: action_size: Dimension of each action\n",
    "        :param: seed: Random seed\n",
    "        :param fc1_units: Number of nodes in first hidden layer\n",
    "        :param fc2_units: Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.elu(self.fc1(state))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        :param: state_size: Dimension of each state\n",
    "        :param: action_size: Dimension of each action\n",
    "        :param: seed: Random seed\n",
    "        :param: fcs1_units: Number of nodes in the first hidden layer\n",
    "        :param: fc2_units: Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.elu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Build the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent\n",
    "BUFFER_SIZE = int(1e5)  \n",
    "BATCH_SIZE = 128        \n",
    "GAMMA = 0.99            \n",
    "TAU = 1e-3              \n",
    "LR_ACTOR = 1e-4 #3e-5 #1e-4         \n",
    "LR_CRITIC = 1e-4 #3e-5 #1e-4        \n",
    "WEIGHT_DECAY_actor = 0.0 #3e-4 #0   \n",
    "WEIGHT_DECAY_critic = 0.0 #1e-6 #0  \n",
    "EPS_START=1.0\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=3e-5\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        :param: state_size: dimension of each state\n",
    "        :param: action_size: dimension of each action\n",
    "        :param: random_seed: random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY_actor)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY_critic)\n",
    "\n",
    "        # Noise process\n",
    "        #self.noise = OUNoise(action_size, random_seed) #single agent only\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed) #both singe and multiple agent\n",
    "        self.eps = EPS_START\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "    \n",
    "        # Make sure target is initialized with the same weight as the source (found on slack to make big difference)\n",
    "        self.hard_update(self.actor_target, self.actor_local)\n",
    "        self.hard_update(self.critic_target, self.critic_local)\n",
    "\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        \n",
    "        # Save experience / reward\n",
    "        # Experience from each agent is separately saved (so it works for single or multi agent environment)\n",
    "        # This works because each agent is operating in a separate/independent environment\n",
    "        for a in range(self.num_agents):\n",
    "            self.memory.add(states[a], actions[a], rewards[a], next_states[a], dones[a])\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        # add noise according to epsilon probability\n",
    "        if add_noise and (np.random.random() < self.eps):\n",
    "            actions += self.noise.sample()\n",
    "            \n",
    "            # update the exploration parameter\n",
    "            self.eps -= EPS_DECAY\n",
    "            if self.eps < EPS_END:\n",
    "                self.eps = EPS_END\n",
    "            # self.noise.reset()\n",
    "\n",
    "        return np.clip(actions, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        \n",
    "            Mathematical foundation:\n",
    "                Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "                where:\n",
    "                    actor_target(state) -> action\n",
    "                    critic_target(state, action) -> Q-value\n",
    "\n",
    "        :param: experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        :param: gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        \n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1.0) #clip the gradient for the critic network (Udacity hint)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        \n",
    "            θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        :param: local_model: PyTorch model (weights will be copied from)\n",
    "        :param: target_model: PyTorch model (weights will be copied to)\n",
    "        :param: tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(source_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        :param: buffer_size (int): maximum size of buffer\n",
    "        :param: batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Interaction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE DEVICE\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# INITIALIZE AGENT\n",
    "agent = Agent(state_size=env_info.vector_observations.shape[1], \n",
    "              action_size=brain.vector_action_space_size, \n",
    "              num_agents=env_info.vector_observations.shape[0],  \n",
    "              random_seed=0)\n",
    "\n",
    "\n",
    "def ddpg(n_episodes=2000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_list = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]     \n",
    "        states = env_info.vector_observations                 \n",
    "        agent.reset()\n",
    "        \n",
    "        scores = np.zeros(num_agents)                         \n",
    "\n",
    "        while True:\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]          \n",
    "        \n",
    "            next_states = env_info.vector_observations        \n",
    "            rewards = env_info.rewards                        \n",
    "            dones = env_info.local_done                       \n",
    "            scores += rewards                                 \n",
    "            \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            # Extra Learning per time step (since generating so much experience at each step)\n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                for _ in range(3):\n",
    "                    experiences = agent.memory.sample()\n",
    "                    agent.learn(experiences, GAMMA)\n",
    "            \n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "            #print('Total score (averaged over agents) this episode: {}'.format(np.mean(score)))\n",
    "        \n",
    "        scores_deque.append(np.mean(scores))\n",
    "        scores_list.append(np.mean(scores))\n",
    "        \n",
    "        #print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {}'.format(i_episode, np.mean(scores_deque), np.mean(scores)))\n",
    "        print('Epsilon: {} and Memory size: {}'.format(agent.eps, len(agent.memory)))\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "        if np.mean(scores_deque) > 30 and len(scores_deque) >= 100:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "    return scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:116: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.55\tScore: 0.549999987706542\n",
      "Epsilon: 0.9703300000000252 and Memory size: 1001\n",
      "Episode 2\tAverage Score: 1.02\tScore: 1.4899999666959047\n",
      "Epsilon: 0.9417400000000495 and Memory size: 2002\n",
      "Episode 3\tAverage Score: 0.99\tScore: 0.9199999794363976\n",
      "Epsilon: 0.914140000000073 and Memory size: 3003\n",
      "Episode 4\tAverage Score: 1.00\tScore: 1.0499999765306711\n",
      "Epsilon: 0.887080000000096 and Memory size: 4004\n",
      "Episode 5\tAverage Score: 1.13\tScore: 1.6599999628961086\n",
      "Epsilon: 0.8605900000001185 and Memory size: 5005\n",
      "Episode 6\tAverage Score: 1.03\tScore: 0.4999999888241291\n",
      "Epsilon: 0.8347600000001405 and Memory size: 6006\n",
      "Episode 7\tAverage Score: 1.05\tScore: 1.1799999736249447\n",
      "Epsilon: 0.8104900000001611 and Memory size: 7007\n",
      "Episode 8\tAverage Score: 1.07\tScore: 1.1899999734014273\n",
      "Epsilon: 0.7868800000001812 and Memory size: 8008\n",
      "Episode 9\tAverage Score: 1.11\tScore: 1.4799999669194221\n",
      "Epsilon: 0.7635400000002011 and Memory size: 9009\n",
      "Episode 10\tAverage Score: 1.13\tScore: 1.2499999720603228\n",
      "Epsilon: 0.7408600000002203 and Memory size: 10010\n",
      "Episode 11\tAverage Score: 1.12\tScore: 1.099999975413084\n",
      "Epsilon: 0.7190500000002389 and Memory size: 11011\n",
      "Episode 12\tAverage Score: 1.04\tScore: 0.05999999865889549\n",
      "Epsilon: 0.6981100000002567 and Memory size: 12012\n",
      "Episode 13\tAverage Score: 1.19\tScore: 3.009999932721257\n",
      "Epsilon: 0.6770500000002746 and Memory size: 13013\n",
      "Episode 14\tAverage Score: 1.20\tScore: 1.2999999709427357\n",
      "Epsilon: 0.6567700000002918 and Memory size: 14014\n",
      "Episode 15\tAverage Score: 1.16\tScore: 0.6899999845772982\n",
      "Epsilon: 0.6373600000003083 and Memory size: 15015\n",
      "Episode 16\tAverage Score: 1.20\tScore: 1.7499999608844519\n",
      "Epsilon: 0.6188500000003241 and Memory size: 16016\n",
      "Episode 17\tAverage Score: 1.16\tScore: 0.5299999881535769\n",
      "Epsilon: 0.5998900000003402 and Memory size: 17017\n",
      "Episode 18\tAverage Score: 1.19\tScore: 1.7099999617785215\n",
      "Epsilon: 0.582460000000355 and Memory size: 18018\n",
      "Episode 19\tAverage Score: 1.18\tScore: 1.0199999772012234\n",
      "Epsilon: 0.5653900000003695 and Memory size: 19019\n",
      "Episode 20\tAverage Score: 1.14\tScore: 0.40999999083578587\n",
      "Epsilon: 0.5481100000003842 and Memory size: 20020\n",
      "Episode 21\tAverage Score: 1.18\tScore: 1.959999956190586\n",
      "Epsilon: 0.5322100000003978 and Memory size: 21021\n",
      "Episode 22\tAverage Score: 1.21\tScore: 1.8499999586492777\n",
      "Epsilon: 0.5158600000004117 and Memory size: 22022\n",
      "Episode 23\tAverage Score: 1.18\tScore: 0.36999999172985554\n",
      "Epsilon: 0.5011000000004242 and Memory size: 23023\n",
      "Episode 24\tAverage Score: 1.17\tScore: 1.149999974295497\n",
      "Epsilon: 0.4859200000004371 and Memory size: 24024\n",
      "Episode 25\tAverage Score: 1.18\tScore: 1.389999968931079\n",
      "Epsilon: 0.47071000000045005 and Memory size: 25025\n",
      "Episode 26\tAverage Score: 1.24\tScore: 2.6899999398738146\n",
      "Epsilon: 0.45682000000046186 and Memory size: 26026\n",
      "Episode 27\tAverage Score: 1.22\tScore: 0.6599999852478504\n",
      "Epsilon: 0.443710000000473 and Memory size: 27027\n",
      "Episode 28\tAverage Score: 1.22\tScore: 1.2499999720603228\n",
      "Epsilon: 0.4309900000004838 and Memory size: 28028\n",
      "Episode 29\tAverage Score: 1.23\tScore: 1.4899999666959047\n",
      "Epsilon: 0.4180900000004948 and Memory size: 29029\n",
      "Episode 30\tAverage Score: 1.24\tScore: 1.4999999664723873\n",
      "Epsilon: 0.4059700000005051 and Memory size: 30030\n",
      "Episode 31\tAverage Score: 1.26\tScore: 2.009999955072999\n",
      "Epsilon: 0.3944200000005149 and Memory size: 31031\n",
      "Episode 32\tAverage Score: 1.28\tScore: 1.9299999568611383\n",
      "Epsilon: 0.3834700000005242 and Memory size: 32032\n",
      "Episode 33\tAverage Score: 1.26\tScore: 0.549999987706542\n",
      "Epsilon: 0.37189000000053407 and Memory size: 33033\n",
      "Episode 34\tAverage Score: 1.33\tScore: 3.5499999206513166\n",
      "Epsilon: 0.3607000000005436 and Memory size: 34034\n",
      "Episode 35\tAverage Score: 1.38\tScore: 3.0699999313801527\n",
      "Epsilon: 0.35017000000055254 and Memory size: 35035\n",
      "Episode 36\tAverage Score: 1.36\tScore: 0.7599999830126762\n",
      "Epsilon: 0.33898000000056205 and Memory size: 36036\n",
      "Episode 37\tAverage Score: 1.39\tScore: 2.3999999463558197\n",
      "Epsilon: 0.32839000000057106 and Memory size: 37037\n",
      "Episode 38\tAverage Score: 1.40\tScore: 1.8799999579787254\n",
      "Epsilon: 0.3192700000005788 and Memory size: 38038\n",
      "Episode 39\tAverage Score: 1.43\tScore: 2.609999941661954\n",
      "Epsilon: 0.31051000000058626 and Memory size: 39039\n",
      "Episode 40\tAverage Score: 1.46\tScore: 2.5299999434500933\n",
      "Epsilon: 0.3007000000005946 and Memory size: 40040\n",
      "Episode 41\tAverage Score: 1.49\tScore: 2.679999940097332\n",
      "Epsilon: 0.2915500000006024 and Memory size: 41041\n",
      "Episode 42\tAverage Score: 1.55\tScore: 4.109999908134341\n",
      "Epsilon: 0.28279000000060983 and Memory size: 42042\n",
      "Episode 43\tAverage Score: 1.58\tScore: 2.7099999394267797\n",
      "Epsilon: 0.2739100000006174 and Memory size: 43043\n",
      "Episode 44\tAverage Score: 1.63\tScore: 3.849999913945794\n",
      "Epsilon: 0.2659000000006242 and Memory size: 44044\n",
      "Episode 45\tAverage Score: 1.66\tScore: 3.109999930486083\n",
      "Epsilon: 0.257920000000631 and Memory size: 45045\n",
      "Episode 46\tAverage Score: 1.68\tScore: 2.299999948590994\n",
      "Epsilon: 0.2500300000006377 and Memory size: 46046\n",
      "Episode 47\tAverage Score: 1.72\tScore: 3.539999920874834\n",
      "Epsilon: 0.24295000000063718 and Memory size: 47047\n",
      "Episode 48\tAverage Score: 1.75\tScore: 3.209999928250909\n",
      "Epsilon: 0.23614000000063667 and Memory size: 48048\n",
      "Episode 49\tAverage Score: 1.72\tScore: 0.4599999897181988\n",
      "Epsilon: 0.22873000000063612 and Memory size: 49049\n",
      "Episode 50\tAverage Score: 1.75\tScore: 3.0299999322742224\n",
      "Epsilon: 0.2219800000006356 and Memory size: 50050\n",
      "Episode 51\tAverage Score: 1.80\tScore: 4.189999906346202\n",
      "Epsilon: 0.2152900000006351 and Memory size: 51051\n",
      "Episode 52\tAverage Score: 1.80\tScore: 1.7299999613314867\n",
      "Epsilon: 0.20890000000063463 and Memory size: 52052\n",
      "Episode 53\tAverage Score: 1.78\tScore: 1.0499999765306711\n",
      "Epsilon: 0.20281000000063418 and Memory size: 53053\n",
      "Episode 54\tAverage Score: 1.80\tScore: 2.699999939650297\n",
      "Epsilon: 0.19729000000063376 and Memory size: 54054\n",
      "Episode 55\tAverage Score: 1.84\tScore: 4.049999909475446\n",
      "Epsilon: 0.19165000000063334 and Memory size: 55055\n",
      "Episode 56\tAverage Score: 1.88\tScore: 4.309999903663993\n",
      "Epsilon: 0.18643000000063295 and Memory size: 56056\n",
      "Episode 57\tAverage Score: 1.90\tScore: 2.659999940544367\n",
      "Epsilon: 0.18079000000063253 and Memory size: 57057\n",
      "Episode 58\tAverage Score: 1.88\tScore: 1.1299999747425318\n",
      "Epsilon: 0.1749700000006321 and Memory size: 58058\n",
      "Episode 59\tAverage Score: 1.92\tScore: 4.279999904334545\n",
      "Epsilon: 0.1698700000006317 and Memory size: 59059\n",
      "Episode 60\tAverage Score: 1.98\tScore: 5.309999881312251\n",
      "Epsilon: 0.16540000000063138 and Memory size: 60060\n",
      "Episode 61\tAverage Score: 1.97\tScore: 1.3199999704957008\n",
      "Epsilon: 0.16081000000063103 and Memory size: 61061\n",
      "Episode 62\tAverage Score: 1.98\tScore: 2.7099999394267797\n",
      "Epsilon: 0.15664000000063072 and Memory size: 62062\n",
      "Episode 63\tAverage Score: 2.00\tScore: 3.419999923557043\n",
      "Epsilon: 0.15190000000063036 and Memory size: 63063\n",
      "Episode 64\tAverage Score: 2.09\tScore: 7.469999833032489\n",
      "Epsilon: 0.14701000000063 and Memory size: 64064\n",
      "Episode 65\tAverage Score: 2.09\tScore: 2.1199999526143074\n",
      "Epsilon: 0.1429300000006297 and Memory size: 65065\n",
      "Episode 66\tAverage Score: 2.12\tScore: 3.6999999172985554\n",
      "Epsilon: 0.1390000000006294 and Memory size: 66066\n",
      "Episode 67\tAverage Score: 2.13\tScore: 2.7899999376386404\n",
      "Epsilon: 0.1349500000006291 and Memory size: 67067\n",
      "Episode 68\tAverage Score: 2.18\tScore: 5.519999876618385\n",
      "Epsilon: 0.13168000000062885 and Memory size: 68068\n",
      "Episode 69\tAverage Score: 2.23\tScore: 5.919999867677689\n",
      "Epsilon: 0.12754000000062854 and Memory size: 69069\n",
      "Episode 70\tAverage Score: 2.29\tScore: 6.4299998562783\n",
      "Epsilon: 0.12340000000062823 and Memory size: 70070\n",
      "Episode 71\tAverage Score: 2.37\tScore: 8.299999814480543\n",
      "Epsilon: 0.11974000000062796 and Memory size: 71071\n",
      "Episode 72\tAverage Score: 2.42\tScore: 5.749999871477485\n",
      "Epsilon: 0.11614000000062769 and Memory size: 72072\n",
      "Episode 73\tAverage Score: 2.47\tScore: 6.289999859407544\n",
      "Epsilon: 0.11251000000062741 and Memory size: 73073\n",
      "Episode 74\tAverage Score: 2.45\tScore: 0.8499999810010195\n",
      "Epsilon: 0.10876000000062713 and Memory size: 74074\n",
      "Episode 75\tAverage Score: 2.52\tScore: 7.58999983035028\n",
      "Epsilon: 0.10579000000062691 and Memory size: 75075\n",
      "Episode 76\tAverage Score: 2.56\tScore: 5.869999868795276\n",
      "Epsilon: 0.10261000000062667 and Memory size: 76076\n",
      "Episode 77\tAverage Score: 2.64\tScore: 8.149999817833304\n",
      "Epsilon: 0.09970000000062645 and Memory size: 77077\n",
      "Episode 78\tAverage Score: 2.67\tScore: 4.9399998895823956\n",
      "Epsilon: 0.09700000000062625 and Memory size: 78078\n",
      "Episode 79\tAverage Score: 2.75\tScore: 9.479999788105488\n",
      "Epsilon: 0.09349000000062599 and Memory size: 79079\n",
      "Episode 80\tAverage Score: 2.75\tScore: 2.869999935850501\n",
      "Epsilon: 0.09076000000062578 and Memory size: 80080\n",
      "Episode 81\tAverage Score: 2.81\tScore: 7.5499998312443495\n",
      "Epsilon: 0.08797000000062558 and Memory size: 81081\n",
      "Episode 82\tAverage Score: 2.86\tScore: 6.6599998511374\n",
      "Epsilon: 0.08518000000062537 and Memory size: 82082\n",
      "Episode 83\tAverage Score: 2.87\tScore: 3.709999917075038\n",
      "Epsilon: 0.08245000000062516 and Memory size: 83083\n",
      "Episode 84\tAverage Score: 2.91\tScore: 5.939999867230654\n",
      "Epsilon: 0.08011000000062499 and Memory size: 84084\n",
      "Episode 85\tAverage Score: 2.96\tScore: 6.989999843761325\n",
      "Epsilon: 0.07774000000062481 and Memory size: 85085\n",
      "Episode 86\tAverage Score: 3.01\tScore: 8.009999820962548\n",
      "Epsilon: 0.07606000000062468 and Memory size: 86086\n",
      "Episode 87\tAverage Score: 3.05\tScore: 6.3299998585134745\n",
      "Epsilon: 0.07396000000062453 and Memory size: 87087\n",
      "Episode 88\tAverage Score: 3.13\tScore: 9.689999783411622\n",
      "Epsilon: 0.07207000000062438 and Memory size: 88088\n",
      "Episode 89\tAverage Score: 3.13\tScore: 3.4399999231100082\n",
      "Epsilon: 0.06982000000062422 and Memory size: 89089\n",
      "Episode 90\tAverage Score: 3.13\tScore: 2.9099999349564314\n",
      "Epsilon: 0.06754000000062405 and Memory size: 90090\n",
      "Episode 91\tAverage Score: 3.13\tScore: 3.279999926686287\n",
      "Epsilon: 0.06538000000062388 and Memory size: 91091\n",
      "Episode 92\tAverage Score: 3.14\tScore: 3.8599999137222767\n",
      "Epsilon: 0.06361000000062375 and Memory size: 92092\n",
      "Episode 93\tAverage Score: 3.16\tScore: 4.809999892488122\n",
      "Epsilon: 0.06199000000062363 and Memory size: 93093\n",
      "Episode 94\tAverage Score: 3.20\tScore: 7.149999840185046\n",
      "Epsilon: 0.06052000000062352 and Memory size: 94094\n",
      "Episode 95\tAverage Score: 3.23\tScore: 5.7799998708069324\n",
      "Epsilon: 0.05848000000062337 and Memory size: 95095\n",
      "Episode 96\tAverage Score: 3.22\tScore: 2.969999933615327\n",
      "Epsilon: 0.05704000000062326 and Memory size: 96096\n",
      "Episode 97\tAverage Score: 3.25\tScore: 5.919999867677689\n",
      "Epsilon: 0.05545000000062314 and Memory size: 97097\n",
      "Episode 98\tAverage Score: 3.26\tScore: 4.579999897629023\n",
      "Epsilon: 0.05389000000062302 and Memory size: 98098\n",
      "Episode 99\tAverage Score: 3.32\tScore: 9.209999794140458\n",
      "Epsilon: 0.052450000000622915 and Memory size: 99099\n",
      "Episode 100\tAverage Score: 3.35\tScore: 5.429999878630042\n",
      "Epsilon: 0.05074000000062279 and Memory size: 100000\n",
      "Episode 100\tAverage Score: 3.35\n",
      "Episode 101\tAverage Score: 3.42\tScore: 7.699999827891588\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 102\tAverage Score: 3.46\tScore: 5.529999876394868\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 103\tAverage Score: 3.51\tScore: 6.469999855384231\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 104\tAverage Score: 3.55\tScore: 4.679999895393848\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 105\tAverage Score: 3.64\tScore: 10.33999976888299\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 106\tAverage Score: 3.72\tScore: 9.09999979659915\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 107\tAverage Score: 3.77\tScore: 6.239999860525131\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 108\tAverage Score: 3.83\tScore: 7.00999984331429\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 109\tAverage Score: 3.92\tScore: 10.06999977491796\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 110\tAverage Score: 3.98\tScore: 7.779999826103449\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 111\tAverage Score: 4.04\tScore: 6.959999844431877\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 112\tAverage Score: 4.11\tScore: 7.109999841079116\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 113\tAverage Score: 4.16\tScore: 7.9799998216331005\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 114\tAverage Score: 4.28\tScore: 13.349999701604247\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 115\tAverage Score: 4.35\tScore: 7.509999832138419\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 116\tAverage Score: 4.40\tScore: 6.859999846667051\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 117\tAverage Score: 4.50\tScore: 10.889999756589532\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 118\tAverage Score: 4.58\tScore: 9.709999782964587\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 119\tAverage Score: 4.69\tScore: 11.099999751895666\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 120\tAverage Score: 4.79\tScore: 10.509999765083194\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 121\tAverage Score: 4.87\tScore: 10.579999763518572\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 122\tAverage Score: 4.94\tScore: 8.339999813586473\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 123\tAverage Score: 5.05\tScore: 11.719999738037586\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 124\tAverage Score: 5.15\tScore: 10.789999758824706\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 125\tAverage Score: 5.22\tScore: 8.319999814033508\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 126\tAverage Score: 5.29\tScore: 10.499999765306711\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 127\tAverage Score: 5.34\tScore: 5.569999875500798\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 128\tAverage Score: 5.46\tScore: 13.029999708756804\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 129\tAverage Score: 5.54\tScore: 9.129999795928597\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 130\tAverage Score: 5.65\tScore: 13.029999708756804\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 131\tAverage Score: 5.73\tScore: 9.309999791905284\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 132\tAverage Score: 5.86\tScore: 15.369999656453729\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 133\tAverage Score: 5.98\tScore: 12.27999972552061\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 134\tAverage Score: 6.07\tScore: 12.579999718815088\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 135\tAverage Score: 6.16\tScore: 11.999999731779099\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 136\tAverage Score: 6.31\tScore: 16.239999637007713\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 137\tAverage Score: 6.48\tScore: 18.69999958202243\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 138\tAverage Score: 6.62\tScore: 16.639999628067017\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 139\tAverage Score: 6.78\tScore: 17.839999601244926\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 140\tAverage Score: 6.93\tScore: 18.12999959476292\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 141\tAverage Score: 7.09\tScore: 18.269999591633677\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 142\tAverage Score: 7.35\tScore: 30.769999312236905\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 143\tAverage Score: 7.43\tScore: 9.939999777823687\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 144\tAverage Score: 7.49\tScore: 9.829999780282378\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 145\tAverage Score: 7.68\tScore: 22.409999499097466\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 146\tAverage Score: 7.93\tScore: 27.789999378845096\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 147\tAverage Score: 8.07\tScore: 17.019999619573355\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 148\tAverage Score: 8.28\tScore: 24.419999454170465\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 149\tAverage Score: 8.47\tScore: 19.31999956816435\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 150\tAverage Score: 8.61\tScore: 17.26999961398542\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 151\tAverage Score: 8.84\tScore: 26.83999940007925\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 152\tAverage Score: 9.08\tScore: 26.09999941661954\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 153\tAverage Score: 9.28\tScore: 20.959999531507492\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 154\tAverage Score: 9.57\tScore: 31.819999288767576\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 155\tAverage Score: 9.70\tScore: 16.53999963030219\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 156\tAverage Score: 9.90\tScore: 24.409999454393983\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 157\tAverage Score: 10.10\tScore: 22.57999949529767\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 158\tAverage Score: 10.38\tScore: 29.539999339729548\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 159\tAverage Score: 10.59\tScore: 25.029999440535903\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 160\tAverage Score: 10.73\tScore: 19.78999955765903\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 161\tAverage Score: 10.96\tScore: 24.189999459311366\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 162\tAverage Score: 11.11\tScore: 17.919999599456787\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 163\tAverage Score: 11.36\tScore: 28.379999365657568\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 164\tAverage Score: 11.58\tScore: 28.88999935425818\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 165\tAverage Score: 11.86\tScore: 30.619999315589666\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 166\tAverage Score: 12.22\tScore: 39.369999120011926\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 167\tAverage Score: 12.54\tScore: 34.47999922931194\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 168\tAverage Score: 12.86\tScore: 37.429999163374305\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 169\tAverage Score: 13.09\tScore: 29.74999933503568\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 170\tAverage Score: 13.40\tScore: 36.849999176338315\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 171\tAverage Score: 13.66\tScore: 34.73999922350049\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 172\tAverage Score: 13.96\tScore: 35.40999920852482\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 173\tAverage Score: 14.20\tScore: 30.299999322742224\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 174\tAverage Score: 14.51\tScore: 31.609999293461442\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 175\tAverage Score: 14.69\tScore: 25.82999942265451\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 176\tAverage Score: 14.92\tScore: 28.559999361634254\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 177\tAverage Score: 14.98\tScore: 14.309999680146575\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 178\tAverage Score: 15.18\tScore: 25.429999431595206\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 179\tAverage Score: 15.35\tScore: 25.819999422878027\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 180\tAverage Score: 15.62\tScore: 29.77999933436513\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 181\tAverage Score: 15.85\tScore: 31.029999306425452\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 182\tAverage Score: 16.12\tScore: 34.06999923847616\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 183\tAverage Score: 16.34\tScore: 25.09999943897128\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 184\tAverage Score: 16.60\tScore: 32.239999279379845\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 185\tAverage Score: 16.83\tScore: 29.43999934196472\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 186\tAverage Score: 17.11\tScore: 36.539999183267355\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 187\tAverage Score: 17.40\tScore: 34.949999218806624\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 188\tAverage Score: 17.60\tScore: 29.759999334812164\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 189\tAverage Score: 17.92\tScore: 35.16999921388924\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 190\tAverage Score: 18.20\tScore: 31.739999290555716\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 191\tAverage Score: 18.51\tScore: 34.41999923065305\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 192\tAverage Score: 18.81\tScore: 32.91999926418066\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 193\tAverage Score: 19.07\tScore: 30.969999307766557\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 194\tAverage Score: 19.32\tScore: 32.49999927356839\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 195\tAverage Score: 19.63\tScore: 37.21999916806817\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 196\tAverage Score: 19.89\tScore: 28.879999354481697\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 197\tAverage Score: 20.02\tScore: 18.799999579787254\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 198\tAverage Score: 20.35\tScore: 37.78999915532768\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 199\tAverage Score: 20.61\tScore: 34.69999922439456\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 200\tAverage Score: 20.92\tScore: 36.23999918997288\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 200\tAverage Score: 20.92\n",
      "Episode 201\tAverage Score: 21.16\tScore: 31.419999297708273\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 202\tAverage Score: 21.35\tScore: 25.13999943807721\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 203\tAverage Score: 21.65\tScore: 36.52999918349087\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 204\tAverage Score: 21.97\tScore: 36.81999917700887\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 205\tAverage Score: 22.17\tScore: 29.999999329447746\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 206\tAverage Score: 22.41\tScore: 33.15999925881624\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 207\tAverage Score: 22.48\tScore: 13.539999697357416\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 208\tAverage Score: 22.72\tScore: 31.109999304637313\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 209\tAverage Score: 22.93\tScore: 30.139999326318502\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 210\tAverage Score: 23.21\tScore: 36.46999918483198\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 211\tAverage Score: 23.48\tScore: 33.95999924093485\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 212\tAverage Score: 23.65\tScore: 24.019999463111162\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 213\tAverage Score: 23.94\tScore: 36.60999918170273\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 214\tAverage Score: 24.15\tScore: 34.82999922148883\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 215\tAverage Score: 24.43\tScore: 35.529999205842614\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 216\tAverage Score: 24.69\tScore: 32.71999926865101\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 217\tAverage Score: 24.92\tScore: 34.26999923400581\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 218\tAverage Score: 25.18\tScore: 34.839999221265316\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 219\tAverage Score: 25.45\tScore: 38.71999913454056\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 220\tAverage Score: 25.72\tScore: 37.17999916896224\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 221\tAverage Score: 26.00\tScore: 38.8699991311878\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 222\tAverage Score: 26.28\tScore: 36.36999918706715\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 223\tAverage Score: 26.39\tScore: 22.759999491274357\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 224\tAverage Score: 26.59\tScore: 30.969999307766557\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 225\tAverage Score: 26.75\tScore: 23.569999473169446\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 226\tAverage Score: 26.94\tScore: 29.74999933503568\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 227\tAverage Score: 26.92\tScore: 4.069999909028411\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 228\tAverage Score: 27.18\tScore: 38.60999913699925\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 229\tAverage Score: 27.36\tScore: 26.929999398067594\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 230\tAverage Score: 27.56\tScore: 33.669999247416854\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 231\tAverage Score: 27.82\tScore: 34.639999225735664\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 232\tAverage Score: 27.93\tScore: 26.129999415948987\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 233\tAverage Score: 27.96\tScore: 16.179999638348818\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 234\tAverage Score: 28.08\tScore: 23.959999464452267\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 235\tAverage Score: 28.31\tScore: 34.97999921813607\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 236\tAverage Score: 28.44\tScore: 29.519999340176582\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 237\tAverage Score: 28.63\tScore: 37.33999916538596\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 238\tAverage Score: 28.75\tScore: 28.77999935671687\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 239\tAverage Score: 28.90\tScore: 33.37999925389886\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 240\tAverage Score: 29.08\tScore: 35.77999920025468\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 241\tAverage Score: 29.03\tScore: 12.969999710097909\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 242\tAverage Score: 29.00\tScore: 28.339999366551638\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 243\tAverage Score: 29.06\tScore: 15.499999653548002\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 244\tAverage Score: 29.21\tScore: 25.28999943472445\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 245\tAverage Score: 29.32\tScore: 32.56999927200377\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 246\tAverage Score: 29.30\tScore: 26.149999415501952\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 247\tAverage Score: 29.37\tScore: 24.139999460428953\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 248\tAverage Score: 29.45\tScore: 32.29999927803874\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 249\tAverage Score: 29.64\tScore: 38.44999914057553\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 250\tAverage Score: 29.53\tScore: 5.929999867454171\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 251\tAverage Score: 29.50\tScore: 24.459999453276396\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 252\tAverage Score: 29.39\tScore: 14.399999678134918\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 253\tAverage Score: 29.44\tScore: 26.01999941840768\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 254\tAverage Score: 29.50\tScore: 38.36999914236367\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 255\tAverage Score: 29.65\tScore: 31.269999301061034\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 256\tAverage Score: 29.74\tScore: 33.869999242946506\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 257\tAverage Score: 29.73\tScore: 21.509999519214034\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 258\tAverage Score: 29.68\tScore: 24.389999454841018\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 259\tAverage Score: 29.70\tScore: 27.359999388456345\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 260\tAverage Score: 29.83\tScore: 32.67999926954508\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 261\tAverage Score: 29.94\tScore: 34.33999923244119\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 262\tAverage Score: 29.99\tScore: 23.839999467134476\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 263\tAverage Score: 30.11\tScore: 39.65999911352992\n",
      "Epsilon: 0.05 and Memory size: 100000\n",
      "Episode 263\tAverage Score: 30.11\n"
     ]
    }
   ],
   "source": [
    "from session import active_session\n",
    "\n",
    "with active_session():\n",
    "    scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd7058d9d30>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#with open(\"scores_file.json\", \"r\") as read_file:\n",
    "#    scores = json.load(read_file)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could eventually used a Prioritized Experience Replay in order to better \"learn\" good trajectories in terms of reward\n",
    "- Try with different algorithms like like A2C, TRPO, PPO or A3.\n",
    "- Fine tune the hyperparameters of the algorithm at all levels. From the neural network architecture, learning rates and decay or even the optimization algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
